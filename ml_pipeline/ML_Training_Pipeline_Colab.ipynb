{\n "cells": [\n {\n "cell_type": "markdown",\n "metadata": {},\n "source": [\n "# HABITUS FAITH - ML Training Pipeline (Google Colab)\",\n "# Complete notebook for training abandonment prediction model\"\n ]\n },\n {\n "cell_type": "markdown",\n "metadata": {},\n "source": [\n "## Cell 1: Introduction and Setup\",\n "This notebook is designed to train a model that predicts habit abandonment based on synthetic data.\",\n "### Requirements\",\n "- None, runs in Colab\",\n "### Expected outputs\",\n "- predictor.tflite, scaler_params.json\",\n "### Estimated time: 5-10 minutes\"\n ]\n },\n {\n "cell_type": "code",\n "execution_count": null,\n "metadata": {},\n "outputs": [],\n "source": [\n "import numpy as np\",\n "import pandas as pd\",\n "from sklearn.model_selection import train_test_split\",\n "from sklearn.preprocessing import StandardScaler\",\n "from sklearn.linear_model import LogisticRegression\",\n "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\",\n "import tensorflow as tf\",\n "from tensorflow import keras\",\n "import json\",\n "import matplotlib.pyplot as plt\",\n "import seaborn as sns\",\n "from datetime import datetime, timedelta\"\n ]\n },\n {\n "cell_type": "code",\n "execution_count": null,\n "metadata": {},\n "outputs": [],\n "source": [\n "def generate_synthetic_training_data(n_records=300):\",\n "    \"\"\"\",\n "    Generate realistic habit completion/abandonment data.\",\n "    \"\"\"\",\n "    np.random.seed(42)\",\n "    records = []\",\n "    \"\"\"\",\n "    Define user archetypes with realistic patterns\",\n "    \"\"\"\",\n "    archetypes = {\",\n "        'successful_weekday_morning': {\",\n "            'weight': 0.25,\",\n "            'hour_range': (6, 9),\",\n "            'preferred_days': [1, 2, 3, 4, 5],\",\n "            'streak_range': (10, 30),\",\n "            'failure_range': (0, 2),\",\n "            'abandonment_rate': 0.10\",\n "        },\",\n "        'struggling_evening': {\",\n "            'weight': 0.20,\",\n "            'hour_range': (20, 23),\",\n "            'preferred_days': [1, 2, 3, 4, 5, 6, 7],\",\n "            'streak_range': (3, 8),\",\n "            'failure_range': (3, 5),\",\n "            'abandonment_rate': 0.60\",\n "        },\",\n "        'weekend_failer': {\",\n "            'weight': 0.15,\",\n "            'hour_range': (10, 22),\",\n "            'preferred_days': [6, 7],\",\n "            'streak_range': (5, 15),\",\n "            'failure_range': (2, 4),\",\n "            'abandonment_rate': 0.75\",\n "        },\",\n "        'inconsistent': {\",\n "            'weight': 0.25,\",\n "            'hour_range': (7, 21),\",\n "            'preferred_days': [1, 2, 3, 4, 5, 6, 7],\",\n "            'streak_range': (0, 5),\",\n "            'failure_range': (4, 7),\",\n "            'abandonment_rate': 0.50\",\n "        },\",\n "        'highly_motivated': {\",\n "            'weight': 0.15,\",\n "            'hour_range': (6, 8),\",\n "            'preferred_days': [1, 2, 3, 4, 5, 6, 7],\",\n "            'streak_range': (15, 30),\",\n "            'failure_range': (0, 1),\",\n "            'abandonment_rate': 0.05\",\n "        }\",\n "    }\",\n "    for i in range(n_records):\",\n "        archetype_name = np.random.choice(\",\n "            list(archetypes.keys()),\",\n "            p=[arch['weight'] for arch in archetypes.values()]\",\n "        )\",\n "        archetype = archetypes[archetype_name]\",\n "        hour = np.random.randint(archetype['hour_range'][0], archetype['hour_range'][1] + 1)\",\n "        day = np.random.choice(archetype['preferred_days'])\",\n "        streak = np.random.randint(archetype['streak_range'][0], archetype['streak_range'][1] + 1)\",\n "        failures = np.random.randint(archetype['failure_range'][0], archetype['failure_range'][1] + 1)\",\n "        hours_from_reminder = np.random.randint(0, 12)\",\n "        base_risk = archetype['abandonment_rate']\",\n "        if hour >= 22:\",\n "            base_risk += 0.15\",\n "        if day in [6, 7]:\",\n "            base_risk += 0.10\",\n "        if failures >= 5:\",\n "            base_risk += 0.20\",\n "        if streak <= 2:\",\n "            base_risk += 0.15\",\n "        if hours_from_reminder > 8:\",\n "            base_risk += 0.10\",\n "        abandoned = 1 if np.random.random() < base_risk else 0\",\n "        records.append({\",\n "            'hourOfDay': hour,\",\n "            'dayOfWeek': day,\",\n "            'streakAtTime': streak,\",\n "            'failuresLast7Days': failures,\",\n "            'hoursFromReminder': hours_from_reminder,\",\n "            'abandoned': abandoned\",\n "        })\",\n "    df = pd.DataFrame(records)\",\n "    return df\"\n ]\n },\n {\n "cell_type": "code",\n "execution_count": null,\n "metadata": {},\n "outputs": [],\n "source": [\n "# Generate data\",\n "print("üîÑ Generating 300 synthetic training records...")\",\n "df = generate_synthetic_training_data(300)\",\n "print(f"‚úÖ Generated {len(df)} records")\",\n "print(f"\nüìä Class distribution:")\",\n "print(f"   - Completed (0): {(~df['abandoned'].astype(bool)).sum()} ({(~df['abandoned'].astype(bool)).sum()/len(df)*100:.1f}%)")\",\n "print(f"   - Abandoned (1): {df['abandoned'].sum()} ({df['abandoned'].sum()/len(df)*100:.1f}%)")\",\n "print(f"\nüìã First 5 records:")\",\n "print(df.head())\"\n ]\n },\n {\n "cell_type": "code",\n "execution_count": null,\n "metadata": {},\n "outputs": [],\n "source": [\n "# Summary statistics\",\n "print("\nüìà Feature Statistics:")\",\n "print(df.describe())\",\n "# Visualizations\",\n "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\",\n "# Hour distribution by abandonment\",\n "axes[0, 0].hist([df[df['abandoned']==0]['hourOfDay'], df[df['abandoned']==1]['hourOfDay']], \",\n "                bins=24, label=['Completed', 'Abandoned'], alpha=0.7)\",\n "axes[0, 0].set_title('Completion Hour Distribution')\",\n "axes[0, 0].set_xlabel('Hour of Day')\",\n "axes[0, 0].legend()\",\n "# Day of week distribution\",\n "axes[0, 1].hist([df[df['abandoned']==0]['dayOfWeek'], df[df['abandoned']==1]['dayOfWeek']], \",\n "                bins=7, label=['Completed', 'Abandoned'], alpha=0.7)\",\n "axes[0, 1].set_title('Day of Week Distribution')\",\n "axes[0, 1].set_xlabel('Day (1=Mon, 7=Sun)')\",\n "axes[0, 1].legend()\",\n "# Streak distribution\",\n "axes[0, 2].hist([df[df['abandoned']==0]['streakAtTime'], df[df['abandoned']==1]['streakAtTime']], \",\n "                bins=20, label=['Completed', 'Abandoned'], alpha=0.7)\",\n "axes[0, 2].set_title('Streak Distribution')\",\n "axes[0, 2].set_xlabel('Streak Length')\",\n "axes[0, 2].legend()\",\n "# Failures distribution\",\n "axes[1, 0].hist([df[df['abandoned']==0]['failuresLast7Days'], df[df['abandoned']==1]['failuresLast7Days']], \",\n "                bins=8, label=['Completed', 'Abandoned'], alpha=0.7)\",\n "axes[1, 0].set_title('Failures Last 7 Days')\",\n "axes[1, 0].set_xlabel('Number of Failures')\",\n "axes[1, 0].legend()\",\n "# Hours from reminder\",\n "axes[1, 1].hist([df[df['abandoned']==0]['hoursFromReminder'], df[df['abandoned']==1]['hoursFromReminder']], \",\n "                bins=12, label=['Completed', 'Abandoned'], alpha=0.7)\",\n "axes[1, 1].set_title('Hours From Reminder')\",\n "axes[1, 1].set_xlabel('Hours')\",\n "axes[1, 1].legend()\",\n "# Correlation heatmap\",\n "axes[1, 2].remove()\",\n "axes[1, 2] = fig.add_subplot(2, 3, 6)\",\n "sns.heatmap(df.corr(), annot=True, fmt='.2f', cmap='coolwarm', ax=axes[1, 2])\",\n "axes[1, 2].set_title('Feature Correlation')\",\n "plt.tight_layout()\",\n "plt.show()\",\n "print("\n‚úÖ Data exploration complete!")\"\n ]\n },\n {\n "cell_type": "code",\n "execution_count": null,\n "metadata": {},\n "outputs": [],\n "source": [\n "print("=" * 60)\",\n "print("üèãÔ∏è TRAINING ML MODEL")\",\n "print("=" * 60)\",\n "# Prepare features and labels\",\n "feature_cols = ['hourOfDay', 'dayOfWeek', 'streakAtTime', 'failuresLast7Days', 'hoursFromReminder']\",\n "X = df[feature_cols].values\",\n "y = df['abandoned'].values\",\n "# Split data\",\n "X_train, X_test, y_train, y_test = train_test_split(\",\n "    X, y, test_size=0.2, random_state=42, stratify=y\",\n ")\",\n "print(f"\nüìä Data split:")\",\n "print(f"   Training: {len(X_train)} samples")\",\n "print(f"   Testing: {len(X_test)} samples")\",\n "# Normalize features\",\n "scaler = StandardScaler()\",\n "X_train_scaled = scaler.fit_transform(X_train)\",\n "X_test_scaled = scaler.transform(X_test)\",\n "# Train Logistic Regression (baseline)\",\n "print(f"\nüîÑ Training Logistic Regression...")\",\n "lr_model = LogisticRegression(max_iter=1000, random_state=42)\",\n "lr_model.fit(X_train_scaled, y_train)\",\n "lr_pred = lr_model.predict(X_test_scaled)\",\n "lr_accuracy = accuracy_score(y_test, lr_pred)\",\n "print(f"‚úÖ Logistic Regression Accuracy: {lr_accuracy:.2%}")\",\n "print(f"\nClassification Report:")\",\n "print(classification_report(y_test, lr_pred, target_names=['Completed', 'Abandoned']))\",\n "# Train Keras Neural Network (for TFLite export)\",\n "print(f"\nüîÑ Training Keras Neural Network...")\",\n "keras_model = keras.Sequential([\",\n "    keras.layers.Input(shape=(5,)),\",\n "    keras.layers.Dense(16, activation='relu'),\",\n "    keras.layers.Dropout(0.2),\",\n "    keras.layers.Dense(8, activation='relu'),\",\n "    keras.layers.Dense(1, activation='sigmoid')\",\n "])\",\n "keras_model.compile(\",\n "    optimizer='adam',\",\n "    loss='binary_crossentropy',\",\n "    metrics=['accuracy']\",\n ")\",\n "early_stopping = keras.callbacks.EarlyStopping(\",\n "    monitor='val_loss',\",\n "    patience=10,\",\n "    restore_best_weights=True\",\n ")\",\n "history = keras_model.fit(\",\n "    X_train_scaled, y_train,\",\n "    validation_data=(X_test_scaled, y_test),\",\n "    epochs=100,\",\n "    batch_size=32,\",\n "    callbacks=[early_stopping],\",\n "    verbose=0\",\n ")\",\n "keras_loss, keras_accuracy = keras_model.evaluate(X_test_scaled, y_test, verbose=0)\",\n "print(f"‚úÖ Keras Model Accuracy: {keras_accuracy:.2%}")\",\n "print(f"   Loss: {keras_loss:.4f}")\",\n "# Training history plot\",\n "plt.figure(figsize=(12, 4))\",\n "plt.subplot(1, 2, 1)\",\n "plt.plot(history.history['loss'], label='Training Loss')\",\n "plt.plot(history.history['val_loss'], label='Validation Loss')\",\n "plt.title('Model Loss')\",\n "plt.xlabel('Epoch')\",\n "plt.ylabel('Loss')\",\n "plt.legend()\",\n "plt.subplot(1, 2, 2)\",\n "plt.plot(history.history['accuracy'], label='Training Accuracy')\",\n "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\",\n "plt.title('Model Accuracy')\",\n "plt.xlabel('Epoch')\",\n "plt.ylabel('Accuracy')\",\n "plt.legend()\",\n "plt.tight_layout()\",\n "plt.show()\",\n "print("\n‚úÖ Training complete!")\"\n ]\n },\n {\n "cell_type": "code",\n "execution_count": null,\n "metadata": {},\n "outputs": [],\n "source": [\n "print("=" * 60)\",\n "print("üì¶ EXPORTING TO TFLITE")\",\n "print("=" * 60)\",\n "# Convert to TFLite\",\n "converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\",\n "converter.optimizations = [tf.lite.Optimize.DEFAULT]\",\n "tflite_model = converter.convert()\",\n "# Save TFLite model\",\n "with open('predictor.tflite', 'wb') as f:\",\n "    f.write(tflite_model)\",\n "tflite_size_mb = len(tflite_model) / (1024 * 1024)\",\n "print(f"‚úÖ TFLite model saved: predictor.tflite")\",\n "print(f"   Size: {tflite_size_mb:.2f} MB")\",\n "# Save scaler parameters\",\n "scaler_params = {\",\n "    'mean': scaler.mean_.tolist(),\",\n "    'scale': scaler.scale_.tolist(),\",\n "} \",\n "with open('scaler_params.json', 'w') as f:\",\n "    json.dump(scaler_params, f, indent=2)\",\n "print(f"‚úÖ Scaler params saved: scaler_params.json")\",\n "# Save training data (for reference)\",\n "df.to_csv('training_data.csv', index=False)\",\n "print(f"‚úÖ Training data saved: training_data.csv")\",\n "print("\n" + "=" * 60)\",\n "print("üéâ EXPORT COMPLETE!")\",\n "print("=" * 60)\",\n "print(f"\nüìÅ Output files:")\",\n "print(f"   1. predictor.tflite ({tflite_size_mb:.2f} MB)")\",\n "print(f"   2. scaler_params.json")\",\n "print(f"   3. training_data.csv (reference)")\"\n ]\n },\n {\n "cell_type": "code",\n "execution_count": null,\n "metadata": {},\n "outputs": [],\n "source": [\n "from google.colab import files\",\n "print("üì• Downloading files...")\",\n "files.download('predictor.tflite')\",\n "files.download('scaler_params.json')\",\n "files.download('training_data.csv')\",\n "print("‚úÖ Downloads initiated! Check your browser's download folder.")\"\n ]\n },\n {\n "cell_type": "markdown",\n "metadata": {},\n "source": [\n "# üöÄ Flutter Integration Instructions\",\n "## 1. Add files to your Flutter project\",\n "Place the downloaded files in:\",\n "````\",\n "assets/ml_models/\",\n "‚îú‚îÄ‚îÄ predictor.tflite\",\n "‚îî‚îÄ‚îÄ scaler_params.json\",\n "````\",\n "## 2. Update pubspec.yaml\",\n "Add to assets:\",\n "```yaml\",\n "flutter:\",\n "  assets:\",\n "    - assets/ml_models/predictor.tflite\",\n "    - assets/ml_models/scaler_params.json\",\n "```\",\n "## 3. Implementation Example\",\n "```dart\",\n "import 'package:tflite_flutter/tflite_flutter.dart';\",\n "class AbandonmentPredictor {\",\n "  Interpreter? _interpreter;\",\n "  Map<String, dynamic>? _scalerParams;\",\n "  Future<void> initialize() async {\",\n "    _interpreter = await Interpreter.fromAsset('assets/ml_models/predictor.tflite');\",\n "    final scalerJson = await rootBundle.loadString('assets/ml_models/scaler_params.json');\",\n "    _scalerParams = json.decode(scalerJson);\",\n "  }\",\n "  Future<double> predictAbandonmentRisk(Habit habit) async {\",\n "    // Prepare input features\",\n "    final input = [\",\n "      [\",\n "        habit.preferredTime?.hour ?? 12,\",\n "        DateTime.now().weekday,\",\n "        habit.currentStreak,\",\n "        habit.failuresLast7Days,\",\n "        habit.hoursSinceLastReminder\",\n "      ]\",\n "    ];\",\n "    // Normalize\",\n "    final normalized = _normalize(input[0]);\",\n "    // Run inference\",\n "    final output = List.filled(1, 0.0).reshape([1, 1]);\",\n "    _interpreter!.run([normalized], output);\",\n "    return output[0][0]; // Returns 0.0-1.0 (abandonment probability)\",\n "  }\",\n "  List<double> _normalize(List<double> features) {\",\n "    final mean = _scalerParams!['mean'] as List;\",\n "    final scale = _scalerParams!['scale'] as List;\",\n "    return List.generate(features.length, (i) => \",\n "      (features[i] - mean[i]) / scale[i]\",\n "    );\",\n "  }\",\n "}\",\n "```\",\n "## 4. Usage in BehavioralEngine\",\n "```dart\",\n "final risk = await abandonmentPredictor.predictAbandonmentRisk(habit);\",\n "if (risk > 0.75) {\",\n "  // High risk - intervene!\",\n "  _reduceDifficulty(habit);\",\n "  _sendMotivationalNotification(habit);\",\n "} else if (risk > 0.50) {\",\n "  // Medium risk - gentle nudge\",\n "  _suggestOptimalTime(habit);\",\n "}\",\n "```\",\n "## 5. Next Steps\",\n "- ‚úÖ Implement AbandonmentPredictor service\",\n "- ‚úÖ Integrate with BehavioralEngine\",\n "- ‚úÖ Add tests (6 realistic scenarios from task)\",\n "- ‚è∞ In 2-3 weeks: Retrain with real user data\",\n "- üìà Monthly retraining for continuous improvement\",\n "## 6. Retraining with Real Data (Future)\",\n "When you have ‚â•50 real user records in Firestore:\",\n "1. Run on your PC:\",\n "```bash\",\n "cd ml_pipeline\",\n "python export_firestore_data.py\",\n "python train_model.py\",\n "```\",\n "2. Replace files in assets/ml_models/\",\n "3. Deploy new app version\",\n "---\",\n "**Model Performance:**\",\n "- Accuracy: {keras_accuracy:.2%}\",\n "- Training records: 300 (synthetic)\",\n "- Features: 5 (hour, day, streak, failures, hours_from_reminder)\",\n "- Output: Abandonment probability (0.0-1.0)\"\n ]\n },\n {\n "cell_type": "code",\n "execution_count": null,\n "metadata": {},\n "outputs": [],\n "source": [\n "# Placeholder for actual execution of model performance\"\n ]\n }\n ],\n "metadata": {\n "kernelspec": {\n "display_name": "Python 3",\n "language": "python",\n "name": "python3"\n },\n "language_info": {\n "codemirror_mode": {\n "name": "ipython",\n "version": 3\n },\n "file_extension": ".py",\n "mimetype": "text/x-python",\n "name": "python",\n "nbconvert_exporter": "python",\n "pygments_lexer": "ipython3",\n "version": "3.8.5"\n }\n },\n "nbformat": 4,\n "nbformat_minor": 4\n}