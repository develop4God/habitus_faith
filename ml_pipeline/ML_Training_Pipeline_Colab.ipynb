{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HABITUS FAITH - ML Training Pipeline (Google Colab)\n",
        "\n",
        "Complete notebook for training abandonment prediction model\n",
        "\n",
        "## Requirements\n",
        "- None, runs in Colab\n",
        "\n",
        "## Expected outputs\n",
        "- predictor.tflite\n",
        "- scaler_params.json\n",
        "\n",
        "## Estimated time: 5-10 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_synthetic_training_data(n_records=300):\n",
        "    np.random.seed(42)\n",
        "    records = []\n",
        "    \n",
        "    archetypes = {\n",
        "        'successful_weekday_morning': {\n",
        "            'weight': 0.25,\n",
        "            'hour_range': (6, 9),\n",
        "            'preferred_days': [1, 2, 3, 4, 5],\n",
        "            'streak_range': (10, 30),\n",
        "            'failure_range': (0, 2),\n",
        "            'abandonment_rate': 0.10\n",
        "        },\n",
        "        'struggling_evening': {\n",
        "            'weight': 0.20,\n",
        "            'hour_range': (20, 23),\n",
        "            'preferred_days': [1, 2, 3, 4, 5, 6, 7],\n",
        "            'streak_range': (3, 8),\n",
        "            'failure_range': (3, 5),\n",
        "            'abandonment_rate': 0.60\n",
        "        },\n",
        "        'weekend_failer': {\n",
        "            'weight': 0.15,\n",
        "            'hour_range': (10, 22),\n",
        "            'preferred_days': [6, 7],\n",
        "            'streak_range': (5, 15),\n",
        "            'failure_range': (2, 4),\n",
        "            'abandonment_rate': 0.75\n",
        "        },\n",
        "        'inconsistent': {\n",
        "            'weight': 0.25,\n",
        "            'hour_range': (7, 21),\n",
        "            'preferred_days': [1, 2, 3, 4, 5, 6, 7],\n",
        "            'streak_range': (0, 5),\n",
        "            'failure_range': (4, 7),\n",
        "            'abandonment_rate': 0.50\n",
        "        },\n",
        "        'highly_motivated': {\n",
        "            'weight': 0.15,\n",
        "            'hour_range': (6, 8),\n",
        "            'preferred_days': [1, 2, 3, 4, 5, 6, 7],\n",
        "            'streak_range': (15, 30),\n",
        "            'failure_range': (0, 1),\n",
        "            'abandonment_rate': 0.05\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    for i in range(n_records):\n",
        "        archetype_name = np.random.choice(\n",
        "            list(archetypes.keys()),\n",
        "            p=[arch['weight'] for arch in archetypes.values()]\n",
        "        )\n",
        "        archetype = archetypes[archetype_name]\n",
        "        \n",
        "        hour = np.random.randint(archetype['hour_range'][0], archetype['hour_range'][1] + 1)\n",
        "        day = np.random.choice(archetype['preferred_days'])\n",
        "        streak = np.random.randint(archetype['streak_range'][0], archetype['streak_range'][1] + 1)\n",
        "        failures = np.random.randint(archetype['failure_range'][0], archetype['failure_range'][1] + 1)\n",
        "        hours_from_reminder = np.random.randint(0, 12)\n",
        "        \n",
        "        base_risk = archetype['abandonment_rate']\n",
        "        if hour >= 22:\n",
        "            base_risk += 0.15\n",
        "        if day in [6, 7]:\n",
        "            base_risk += 0.10\n",
        "        if failures >= 5:\n",
        "            base_risk += 0.20\n",
        "        if streak <= 2:\n",
        "            base_risk += 0.15\n",
        "        if hours_from_reminder > 8:\n",
        "            base_risk += 0.10\n",
        "        \n",
        "        abandoned = 1 if np.random.random() < base_risk else 0\n",
        "        \n",
        "        records.append({\n",
        "            'hourOfDay': hour,\n",
        "            'dayOfWeek': day,\n",
        "            'streakAtTime': streak,\n",
        "            'failuresLast7Days': failures,\n",
        "            'hoursFromReminder': hours_from_reminder,\n",
        "            'abandoned': abandoned\n",
        "        })\n",
        "    \n",
        "    df = pd.DataFrame(records)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Generating 300 synthetic training records...')\n",
        "df = generate_synthetic_training_data(300)\n",
        "print(f'Generated {len(df)} records')\n",
        "print(f'\\nClass distribution:')\n",
        "print(f\"   - Completed (0): {(~df['abandoned'].astype(bool)).sum()} ({(~df['abandoned'].astype(bool)).sum()/len(df)*100:.1f}%)\")\n",
        "print(f\"   - Abandoned (1): {df['abandoned'].sum()} ({df['abandoned'].sum()/len(df)*100:.1f}%)\")\n",
        "print(f'\\nFirst 5 records:')\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\nFeature Statistics:')\n",
        "print(df.describe())\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "axes[0, 0].hist([df[df['abandoned']==0]['hourOfDay'], df[df['abandoned']==1]['hourOfDay']], \n",
        "                bins=24, label=['Completed', 'Abandoned'], alpha=0.7)\n",
        "axes[0, 0].set_title('Completion Hour Distribution')\n",
        "axes[0, 0].set_xlabel('Hour of Day')\n",
        "axes[0, 0].legend()\n",
        "\n",
        "axes[0, 1].hist([df[df['abandoned']==0]['dayOfWeek'], df[df['abandoned']==1]['dayOfWeek']], \n",
        "                bins=7, label=['Completed', 'Abandoned'], alpha=0.7)\n",
        "axes[0, 1].set_title('Day of Week Distribution')\n",
        "axes[0, 1].set_xlabel('Day (1=Mon, 7=Sun)')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "axes[0, 2].hist([df[df['abandoned']==0]['streakAtTime'], df[df['abandoned']==1]['streakAtTime']], \n",
        "                bins=20, label=['Completed', 'Abandoned'], alpha=0.7)\n",
        "axes[0, 2].set_title('Streak Distribution')\n",
        "axes[0, 2].set_xlabel('Streak Length')\n",
        "axes[0, 2].legend()\n",
        "\n",
        "axes[1, 0].hist([df[df['abandoned']==0]['failuresLast7Days'], df[df['abandoned']==1]['failuresLast7Days']], \n",
        "                bins=8, label=['Completed', 'Abandoned'], alpha=0.7)\n",
        "axes[1, 0].set_title('Failures Last 7 Days')\n",
        "axes[1, 0].set_xlabel('Number of Failures')\n",
        "axes[1, 0].legend()\n",
        "\n",
        "axes[1, 1].hist([df[df['abandoned']==0]['hoursFromReminder'], df[df['abandoned']==1]['hoursFromReminder']], \n",
        "                bins=12, label=['Completed', 'Abandoned'], alpha=0.7)\n",
        "axes[1, 1].set_title('Hours From Reminder')\n",
        "axes[1, 1].set_xlabel('Hours')\n",
        "axes[1, 1].legend()\n",
        "\n",
        "sns.heatmap(df.corr(), annot=True, fmt='.2f', cmap='coolwarm', ax=axes[1, 2])\n",
        "axes[1, 2].set_title('Feature Correlation')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print('\\nData exploration complete!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('='*60)\n",
        "print('TRAINING ML MODEL')\n",
        "print('='*60)\n",
        "\n",
        "feature_cols = ['hourOfDay', 'dayOfWeek', 'streakAtTime', 'failuresLast7Days', 'hoursFromReminder']\n",
        "X = df[feature_cols].values\n",
        "y = df['abandoned'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f'\\nData split:')\n",
        "print(f'   Training: {len(X_train)} samples')\n",
        "print(f'   Testing: {len(X_test)} samples')\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f'\\nTraining Logistic Regression...')\n",
        "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "lr_model.fit(X_train_scaled, y_train)\n",
        "lr_pred = lr_model.predict(X_test_scaled)\n",
        "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
        "\n",
        "print(f'Logistic Regression Accuracy: {lr_accuracy:.2%}')\n",
        "print(f'\\nClassification Report:')\n",
        "print(classification_report(y_test, lr_pred, target_names=['Completed', 'Abandoned']))\n",
        "\n",
        "print(f'\\nTraining Keras Neural Network...')\n",
        "keras_model = keras.Sequential([\n",
        "    keras.layers.Input(shape=(5,)),\n",
        "    keras.layers.Dense(16, activation='relu'),\n",
        "    keras.layers.Dropout(0.2),\n",
        "    keras.layers.Dense(8, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "keras_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history = keras_model.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    validation_data=(X_test_scaled, y_test),\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "keras_loss, keras_accuracy = keras_model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "print(f'Keras Model Accuracy: {keras_accuracy:.2%}')\n",
        "print(f'   Loss: {keras_loss:.4f}')\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "print('\\nTraining complete!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('='*60)\n",
        "print('EXPORTING TO TFLITE')\n",
        "print('='*60)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('predictor.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "tflite_size_mb = len(tflite_model) / (1024 * 1024)\n",
        "print(f'TFLite model saved: predictor.tflite')\n",
        "print(f'   Size: {tflite_size_mb:.2f} MB')\n",
        "\n",
        "scaler_params = {\n",
        "    'mean': scaler.mean_.tolist(),\n",
        "    'scale': scaler.scale_.tolist()\n",
        "}\n",
        "\n",
        "with open('scaler_params.json', 'w') as f:\n",
        "    json.dump(scaler_params, f, indent=2)\n",
        "\n",
        "print(f'Scaler params saved: scaler_params.json')\n",
        "\n",
        "df.to_csv('training_data.csv', index=False)\n",
        "print(f'Training data saved: training_data.csv')\n",
        "\n",
        "print('\\n' + '='*60)\n",
        "print('EXPORT COMPLETE!')\n",
        "print('='*60)\n",
        "print(f'\\nOutput files:')\n",
        "print(f'   1. predictor.tflite ({tflite_size_mb:.2f} MB)')\n",
        "print(f'   2. scaler_params.json')\n",
        "print(f'   3. training_data.csv (reference)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "print('Downloading files...')\n",
        "files.download('predictor.tflite')\n",
        "files.download('scaler_params.json')\n",
        "files.download('training_data.csv')\n",
        "print('Downloads initiated! Check your browser download folder.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Flutter Integration Instructions\n",
        "\n",
        "## 1. Add files to your Flutter project\n",
        "\n",
        "Place the downloaded files in:\n",
        "\n",
        "```\n",
        "assets/ml_models/\n",
        "├── predictor.tflite\n",
        "└── scaler_params.json\n",
        "```\n",
        "\n",
        "## 2. Update pubspec.yaml\n",
        "\n",
        "```yaml\n",
        "flutter:\n",
        "  assets:\n",
        "    - assets/ml_models/predictor.tflite\n",
        "    - assets/ml_models/scaler_params.json\n",
        "```\n",
        "\n",
        "## 3. Add dependency\n",
        "\n",
        "```yaml\n",
        "dependencies:\n",
        "  tflite_flutter: ^0.10.4\n",
        "```\n",
        "\n",
        "## 4. Implementation Example\n",
        "\n",
        "```dart\n",
        "import 'package:tflite_flutter/tflite_flutter.dart';\n",
        "import 'dart:convert';\n",
        "import 'package:flutter/services.dart';\n",
        "\n",
        "class AbandonmentPredictor {\n",
        "  Interpreter? _interpreter;\n",
        "  Map<String, dynamic>? _scalerParams;\n",
        "\n",
        "  Future<void> initialize() async {\n",
        "    _interpreter = await Interpreter.fromAsset('assets/ml_models/predictor.tflite');\n",
        "    final scalerJson = await rootBundle.loadString('assets/ml_models/scaler_params.json');\n",
        "    _scalerParams = json.decode(scalerJson);\n",
        "  }\n",
        "\n",
        "  Future<double> predictAbandonmentRisk(Habit habit) async {\n",
        "    final input = [\n",
        "      [\n",
        "        (habit.preferredTime?.hour ?? 12).toDouble(),\n",
        "        DateTime.now().weekday.toDouble(),\n",
        "        habit.currentStreak.toDouble(),\n",
        "        habit.failuresLast7Days.toDouble(),\n",
        "        habit.hoursSinceLastReminder.toDouble()\n",
        "      ]\n",
        "    ];\n",
        "\n",
        "    final normalized = _normalize(input[0]);\n",
        "    final output = List.filled(1, 0.0).reshape([1, 1]);\n",
        "    _interpreter!.run([normalized], output);\n",
        "\n",
        "    return output[0][0];\n",
        "  }\n",
        "\n",
        "  List<double> _normalize(List<double> features) {\n",
        "    final mean = (_scalerParams!['mean'] as List).cast<double>();\n",
        "    final scale = (_scalerParams!['scale'] as List).cast<double>();\n",
        "    return List.generate(features.length, (i) => (features[i] - mean[i]) / scale[i]);\n",
        "  }\n",
        "\n",
        "  void dispose() {\n",
        "    _interpreter?.close();\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "## 5. Usage in BehavioralEngine\n",
        "\n",
        "```dart\n",
        "final predictor = AbandonmentPredictor();\n",
        "await predictor.initialize();\n",
        "\n",
        "final risk = await predictor.predictAbandonmentRisk(habit);\n",
        "\n",
        "if (risk > 0.75) {\n",
        "  // High risk - intervene!\n",
        "  _reduceDifficulty(habit);\n",
        "  _sendMotivationalNotification(habit);\n",
        "} else if (risk > 0.50) {\n",
        "  // Medium risk - gentle nudge\n",
        "  _suggestOptimalTime(habit);\n",
        "}\n",
        "```\n",
        "\n",
        "## 6. Next Steps\n",
        "\n",
        "- Implement AbandonmentPredictor service\n",
        "- Integrate with BehavioralEngine\n",
        "- Add tests (6 realistic scenarios)\n",
        "- In 2-3 weeks: Retrain with real user data\n",
        "- Monthly retraining for continuous improvement\n",
        "\n",
        "## 7. Retraining with Real Data (Future)\n",
        "\n",
        "When you have ≥50 real user records in Firestore:\n",
        "\n",
        "```bash\n",
        "cd ml_pipeline\n",
        "python export_firestore_data.py\n",
        "python train_model.py\n",
        "```\n",
        "\n",
        "Then replace files in assets/ml_models/ and deploy new version."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
